{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "df=pd.read_csv('fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        emotion                                             pixels        Usage\n",
       "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
       "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
       "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
       "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
       "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
       "5            2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...     Training\n",
       "6            4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...     Training\n",
       "7            3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...     Training\n",
       "8            3  85 84 90 121 101 102 133 153 153 169 177 189 1...     Training\n",
       "9            2  255 254 255 254 254 179 122 107 95 124 149 150...     Training\n",
       "10           0  30 24 21 23 25 25 49 67 84 103 120 125 130 139...     Training\n",
       "11           6  39 75 78 58 58 45 49 48 103 156 81 45 41 38 49...     Training\n",
       "12           6  219 213 206 202 209 217 216 215 219 218 223 23...     Training\n",
       "13           6  148 144 130 129 119 122 129 131 139 153 140 12...     Training\n",
       "14           3  4 2 13 41 56 62 67 87 95 62 65 70 80 107 127 1...     Training\n",
       "15           5  107 107 109 109 109 109 110 101 123 140 144 14...     Training\n",
       "16           3  14 14 18 28 27 22 21 30 42 61 77 86 88 95 100 ...     Training\n",
       "17           2  255 255 255 255 255 255 255 255 255 255 255 25...     Training\n",
       "18           6  134 124 167 180 197 194 203 210 204 203 209 20...     Training\n",
       "19           4  219 192 179 148 208 254 192 98 121 103 145 185...     Training\n",
       "20           4  1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 7 12 23 45 38 ...     Training\n",
       "21           2  174 51 37 37 38 41 22 25 22 24 35 51 70 83 98 ...     Training\n",
       "22           0  123 125 124 142 209 226 234 236 231 232 235 22...     Training\n",
       "23           0  8 9 14 21 26 32 37 46 52 62 72 70 71 73 76 83 ...     Training\n",
       "24           3  252 250 246 229 182 140 98 72 53 44 67 95 95 8...     Training\n",
       "25           3  224 227 219 217 215 210 187 177 189 200 206 21...     Training\n",
       "26           5  162 200 187 180 197 198 196 192 176 152 136 11...     Training\n",
       "27           0  236 230 225 226 228 209 199 193 196 211 199 19...     Training\n",
       "28           3  210 210 210 210 211 207 147 103 68 60 47 70 12...     Training\n",
       "29           5  50 44 74 141 187 187 169 113 80 128 181 172 76...     Training\n",
       "...        ...                                                ...          ...\n",
       "35857        5  253 255 229 150 89 61 54 60 55 49 61 50 56 45 ...  PrivateTest\n",
       "35858        4  11 11 11 13 20 27 38 41 38 34 20 13 10 39 85 1...  PrivateTest\n",
       "35859        4  11 13 16 27 24 26 89 161 190 197 201 206 210 2...  PrivateTest\n",
       "35860        3  27 42 62 91 112 118 122 123 119 124 129 131 13...  PrivateTest\n",
       "35861        6  233 232 208 188 194 179 177 167 157 180 185 19...  PrivateTest\n",
       "35862        2  73 54 63 76 82 71 67 69 73 72 92 98 117 119 14...  PrivateTest\n",
       "35863        5  196 196 197 197 198 198 198 196 176 148 122 10...  PrivateTest\n",
       "35864        4  68 59 65 78 118 131 137 141 142 135 135 137 13...  PrivateTest\n",
       "35865        3  102 109 109 106 104 107 112 109 116 119 117 12...  PrivateTest\n",
       "35866        6  87 82 59 61 72 102 143 130 90 95 143 173 146 1...  PrivateTest\n",
       "35867        3  198 198 197 196 196 197 196 196 196 195 196 18...  PrivateTest\n",
       "35868        2  204 209 215 218 214 214 214 217 205 175 170 16...  PrivateTest\n",
       "35869        3  217 220 222 223 223 224 225 223 223 225 223 22...  PrivateTest\n",
       "35870        2  6 8 4 5 30 48 61 70 76 79 98 117 130 137 143 1...  PrivateTest\n",
       "35871        6  112 102 98 89 98 133 164 185 180 179 185 169 1...  PrivateTest\n",
       "35872        5  131 159 90 59 10 0 1 1 1 0 1 1 0 0 2 2 5 7 9 1...  PrivateTest\n",
       "35873        4  54 57 77 122 121 76 73 80 58 22 26 27 35 41 66...  PrivateTest\n",
       "35874        5  43 43 51 73 94 97 102 95 99 107 126 144 154 17...  PrivateTest\n",
       "35875        5  248 251 239 144 102 95 82 77 91 138 153 145 14...  PrivateTest\n",
       "35876        6  29 29 27 31 49 56 29 19 22 20 34 43 55 71 85 9...  PrivateTest\n",
       "35877        6  139 143 145 154 159 168 176 181 190 191 195 19...  PrivateTest\n",
       "35878        3  0 39 81 80 104 97 51 64 68 46 41 67 53 68 70 5...  PrivateTest\n",
       "35879        2  0 0 6 16 19 31 47 18 26 19 17 8 15 3 4 2 14 20...  PrivateTest\n",
       "35880        2  164 172 175 171 172 173 178 181 188 192 197 20...  PrivateTest\n",
       "35881        0  181 177 176 156 178 144 136 132 122 107 131 16...  PrivateTest\n",
       "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
       "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
       "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
       "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
       "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
       "\n",
       "[35887 rows x 3 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      "emotion    35887 non-null int64\n",
      "pixels     35887 non-null object\n",
      "Usage      35887 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,train_y,X_test,test_y=[],[],[],[]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    val=row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "           X_train.append(np.array(val,'float32'))\n",
    "           train_y.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "           X_test.append(np.array(val,'float32'))\n",
    "           test_y.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 128\n",
    "epochs = 45\n",
    "width, height = 48, 48\n",
    "\n",
    "X_train = np.array(X_train,'float32')\n",
    "train_y = np.array(train_y,'float32')\n",
    "X_test = np.array(X_test,'float32')\n",
    "test_y = np.array(test_y,'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)\n",
    "test_y=np_utils.to_categorical(test_y, num_classes=num_labels)\n",
    "test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6505896 ,  1.7122597 ,  1.774942  , ..., -0.9225547 ,\n",
       "         0.18117245,  0.80417955],\n",
       "       [ 0.45219082,  0.84190166,  1.0658671 , ...,  0.73589116,\n",
       "         0.6640275 ,  0.5656745 ],\n",
       "       [-0.6116937 ,  0.02127829, -0.6688345 , ..., -0.33572   ,\n",
       "        -0.35250947, -0.32558146],\n",
       "       ...,\n",
       "       [ 1.6628181 ,  1.7246933 ,  1.7876041 , ..., -0.84601104,\n",
       "        -0.8226578 , -0.87790906],\n",
       "       [-1.0519217 , -1.1350546 , -1.0486962 , ..., -1.4073311 ,\n",
       "        -1.3944598 , -1.4051309 ],\n",
       "       [-0.7095221 , -0.66257447, -0.6941586 , ..., -0.01678811,\n",
       "         0.6386141 ,  0.80417955]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train -= np.mean(X_train, axis=0)\n",
    "X_train /= np.std(X_train, axis=0)\n",
    "X_test -= np.mean(X_test, axis=0)\n",
    "X_test /= np.std(X_test, axis=0)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
    "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x21452f98c50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected neural networks\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "225/225 [==============================] - 321s 1s/step - loss: 1.7807 - accuracy: 0.2603 - val_loss: 1.7250 - val_accuracy: 0.3140\n",
      "Epoch 2/45\n",
      "225/225 [==============================] - 332s 1s/step - loss: 1.5954 - accuracy: 0.3649 - val_loss: 1.4735 - val_accuracy: 0.4260\n",
      "Epoch 3/45\n",
      "225/225 [==============================] - 297s 1s/step - loss: 1.4584 - accuracy: 0.4308 - val_loss: 1.3686 - val_accuracy: 0.4723\n",
      "Epoch 4/45\n",
      "225/225 [==============================] - 296s 1s/step - loss: 1.3785 - accuracy: 0.4680 - val_loss: 1.2911 - val_accuracy: 0.4870\n",
      "Epoch 5/45\n",
      "225/225 [==============================] - 294s 1s/step - loss: 1.3193 - accuracy: 0.4913 - val_loss: 1.2708 - val_accuracy: 0.5104\n",
      "Epoch 6/45\n",
      "225/225 [==============================] - 292s 1s/step - loss: 1.2701 - accuracy: 0.5092 - val_loss: 1.2207 - val_accuracy: 0.5297\n",
      "Epoch 7/45\n",
      "225/225 [==============================] - 292s 1s/step - loss: 1.2395 - accuracy: 0.5246 - val_loss: 1.2233 - val_accuracy: 0.5308\n",
      "Epoch 8/45\n",
      "225/225 [==============================] - 294s 1s/step - loss: 1.2054 - accuracy: 0.5388 - val_loss: 1.1952 - val_accuracy: 0.5347\n",
      "Epoch 9/45\n",
      "225/225 [==============================] - 295s 1s/step - loss: 1.1797 - accuracy: 0.5492 - val_loss: 1.1827 - val_accuracy: 0.5536\n",
      "Epoch 10/45\n",
      "225/225 [==============================] - 294s 1s/step - loss: 1.1569 - accuracy: 0.5554 - val_loss: 1.1825 - val_accuracy: 0.5500\n",
      "Epoch 11/45\n",
      "225/225 [==============================] - 295s 1s/step - loss: 1.1329 - accuracy: 0.5645 - val_loss: 1.1767 - val_accuracy: 0.5528\n",
      "Epoch 12/45\n",
      "225/225 [==============================] - 291s 1s/step - loss: 1.1134 - accuracy: 0.5762 - val_loss: 1.1630 - val_accuracy: 0.5495\n",
      "Epoch 13/45\n",
      "225/225 [==============================] - 296s 1s/step - loss: 1.0906 - accuracy: 0.5842 - val_loss: 1.1373 - val_accuracy: 0.5626\n",
      "Epoch 14/45\n",
      "225/225 [==============================] - 291s 1s/step - loss: 1.0736 - accuracy: 0.5920 - val_loss: 1.1523 - val_accuracy: 0.5614\n",
      "Epoch 15/45\n",
      "225/225 [==============================] - 293s 1s/step - loss: 1.0468 - accuracy: 0.6000 - val_loss: 1.1463 - val_accuracy: 0.5692\n",
      "Epoch 16/45\n",
      "225/225 [==============================] - 293s 1s/step - loss: 1.0377 - accuracy: 0.6059 - val_loss: 1.1324 - val_accuracy: 0.5665\n",
      "Epoch 17/45\n",
      "225/225 [==============================] - 294s 1s/step - loss: 1.0211 - accuracy: 0.6093 - val_loss: 1.1451 - val_accuracy: 0.5645\n",
      "Epoch 18/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.9999 - accuracy: 0.6162 - val_loss: 1.1647 - val_accuracy: 0.5612\n",
      "Epoch 19/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.9767 - accuracy: 0.6272 - val_loss: 1.1503 - val_accuracy: 0.5667\n",
      "Epoch 20/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.9557 - accuracy: 0.6337 - val_loss: 1.1420 - val_accuracy: 0.5740\n",
      "Epoch 21/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.9362 - accuracy: 0.6424 - val_loss: 1.1567 - val_accuracy: 0.5709\n",
      "Epoch 22/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.9268 - accuracy: 0.6468 - val_loss: 1.1545 - val_accuracy: 0.5704\n",
      "Epoch 23/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.9067 - accuracy: 0.6561 - val_loss: 1.1510 - val_accuracy: 0.5759\n",
      "Epoch 24/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.8860 - accuracy: 0.6638 - val_loss: 1.1567 - val_accuracy: 0.5726\n",
      "Epoch 25/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.8644 - accuracy: 0.6717 - val_loss: 1.1546 - val_accuracy: 0.5712\n",
      "Epoch 26/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.8481 - accuracy: 0.6774 - val_loss: 1.1926 - val_accuracy: 0.5642\n",
      "Epoch 27/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.8331 - accuracy: 0.6854 - val_loss: 1.1999 - val_accuracy: 0.5759\n",
      "Epoch 28/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.8141 - accuracy: 0.6890 - val_loss: 1.1782 - val_accuracy: 0.5759\n",
      "Epoch 29/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.7944 - accuracy: 0.6972 - val_loss: 1.2062 - val_accuracy: 0.5815\n",
      "Epoch 30/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.7882 - accuracy: 0.7020 - val_loss: 1.2162 - val_accuracy: 0.5695\n",
      "Epoch 31/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.7618 - accuracy: 0.7112 - val_loss: 1.2041 - val_accuracy: 0.5821\n",
      "Epoch 32/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.7573 - accuracy: 0.7168 - val_loss: 1.2314 - val_accuracy: 0.5885\n",
      "Epoch 33/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.7322 - accuracy: 0.7267 - val_loss: 1.2585 - val_accuracy: 0.5790\n",
      "Epoch 34/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.7263 - accuracy: 0.7255 - val_loss: 1.2532 - val_accuracy: 0.5715\n",
      "Epoch 35/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.7118 - accuracy: 0.7322 - val_loss: 1.2720 - val_accuracy: 0.5807\n",
      "Epoch 36/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.6896 - accuracy: 0.7395 - val_loss: 1.3037 - val_accuracy: 0.5770\n",
      "Epoch 37/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.6798 - accuracy: 0.7443 - val_loss: 1.2953 - val_accuracy: 0.5776\n",
      "Epoch 38/45\n",
      "225/225 [==============================] - 291s 1s/step - loss: 0.6717 - accuracy: 0.7469 - val_loss: 1.3134 - val_accuracy: 0.5804\n",
      "Epoch 39/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.6486 - accuracy: 0.7582 - val_loss: 1.2734 - val_accuracy: 0.5798\n",
      "Epoch 40/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.6415 - accuracy: 0.7588 - val_loss: 1.3538 - val_accuracy: 0.5768\n",
      "Epoch 41/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.6281 - accuracy: 0.7633 - val_loss: 1.3737 - val_accuracy: 0.5812\n",
      "Epoch 42/45\n",
      "225/225 [==============================] - 292s 1s/step - loss: 0.6269 - accuracy: 0.7656 - val_loss: 1.3499 - val_accuracy: 0.5823\n",
      "Epoch 43/45\n",
      "225/225 [==============================] - 290s 1s/step - loss: 0.6016 - accuracy: 0.7780 - val_loss: 1.3463 - val_accuracy: 0.5765\n",
      "Epoch 44/45\n",
      "225/225 [==============================] - 291s 1s/step - loss: 0.5948 - accuracy: 0.7774 - val_loss: 1.4010 - val_accuracy: 0.5790\n",
      "Epoch 45/45\n",
      "225/225 [==============================] - 289s 1s/step - loss: 0.5852 - accuracy: 0.7834 - val_loss: 1.3877 - val_accuracy: 0.5843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2140300c240>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Training the model\n",
    "model.fit(X_train, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, test_y),\n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5d59026e3541>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfer_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fer.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfer_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fer.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "fer_json = model.to_json()\n",
    "with open(\"fer.json\", \"w\") as json_file:\n",
    "    json_file.write(fer_json)\n",
    "model.save_weights(\"fer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"fer.json\") as datafile:\n",
    "  #  data=json.load(datafile)\n",
    "  #  dataframe=pd.DataFrame(data)\n",
    "#pandas.read_json(\"fer.json\")\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#load model\n",
    "model = model_from_json(open(\"fer.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('fer.h5')\n",
    "\n",
    "\n",
    "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "\n",
    "\n",
    "    for (x,y,w,h) in faces_detected:\n",
    "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image\n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "        img_pixels /= 255\n",
    "\n",
    "        predictions = model.predict(img_pixels)\n",
    "\n",
    "        #find max indexed array\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion = emotions[max_index]\n",
    "\n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    resized_img = cv2.resize(test_img, (1000, 700))\n",
    "    cv2.imshow('Facial emotion analysis ',resized_img)\n",
    "\n",
    "\n",
    "\n",
    "    if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
